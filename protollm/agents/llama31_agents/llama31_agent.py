import ast
import json
import logging

import requests
from tornado.gen import sleep

from protollm.agents.agent_response_parsers.agent_response_parser import AgentResponseParser
from protollm.agents.llama31_agents.llm_agent import LargeLanguageModelAgent


class Llama31Agent(LargeLanguageModelAgent):
    """Class for interacting with the llama.cpp LLM API to create a conversational
    agent.
    Currently only supports the chat/completions endpoint with llama 3.1 roles.
    Attributes:
        api_key (str): your llama.cpp API key.
        base_url (str): your llama.cpp API base URL
            (e.g. "https://api.llama.cpp.com/v1").
        tools_module (str): the module containing the tools
            to be used by the agent (e.g. "my_lib.tools").
        model (str): the model to be used by the agent
            (e.g. "meta-llama/llama-3.1-70b-instruct").
        response_parser (AgentResponseParser): a response parser to extract function calls
        custom_system_message (str): a system role message with clear instructions
            for the LLM agent regarding its roles.
        custom_user_message (str): a user role message with clear instructions
            for the output format expected from the agent.
        functions_metadata (list): a list of dictionaries containing metadata
            for the tools to be used by the agent.
        temperature (float): the temperature to be used by the agent.
        max_tokens (int): the maximum number of tokens generated by the agent.
        context (list): stores context conversation history.
    """
    def __init__(
        self,
        api_key: str,
        base_url: str,
        tools_module: str,
        model: str,
        response_parser: AgentResponseParser,
        custom_system_message: str,
        custom_user_message: str,
        functions_metadata: list,  # TODO: Make decorator to parse functions metadata
        temperature: float,
        max_tokens: int,
    ):
        """
        Creates an instance of the Llama31Agent class.
        Args:
            api_key:
            base_url:
            tools_module:
            model:
            custom_system_message:
            custom_user_message:
            functions_metadata:
            temperature:
            max_tokens:
        """
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.tools_module = tools_module
        self.response_parser = response_parser
        self.custom_system_message = custom_system_message
        self.custom_user_message = custom_user_message
        self.functions_metadata = functions_metadata
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.context = []

        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)

        self._add_to_context(
            self.custom_system_message + "\n" + str(self.functions_metadata),
            role="system",
        )

    def _add_to_context(self, message, role="user"):
        self.context.append({"role": role, "content": message})

    def _get_response(self):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.model,
            "messages": self.context,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        sleep(1)
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            data=json.dumps(payload),
        )

        if response.status_code == 200:
            assistant_response = response.json()["choices"][0]["message"]["content"]
            self._add_to_context(assistant_response, role="assistant")
            self.logger.info("REQUEST: %s", payload)
            self.logger.info("RESPONSE: %s", assistant_response)
            return assistant_response
        else:
            response.raise_for_status()

    def _process_response(self, response):
        parser = self.response_parser
        try:
            parsed_values = parser.parse_function_call(response)
            match parsed_values:
                case list():  # Multiple function calls
                    for function_call in parsed_values:
                        function_name, parameters = function_call
                        result = self._execute_function(
                            self.tools_module, function_name, parameters
                        )
                        self._add_to_context(str(result), role="ipython")
                case tuple():  # Single function call
                    function_name, parameters = parsed_values
                    result = self._execute_function(
                        self.tools_module, function_name, parameters
                    )
                    self._add_to_context(str(result), role="ipython")
        except Exception as e:
            error_message = f"Error executing tool: {str(e)}"
            self.logger.error(error_message)
            self._add_to_context(error_message, role="ipython")

    def loop_until_satisfactory(self, user_prompt):
        self._add_to_context(
            self.custom_user_message + "\n" + "Question: " + user_prompt,
            role="user",
        )

        while True:
            response = self._get_response()
            try:
                response_literal = ast.literal_eval(response)
            except SyntaxError:
                break

            match response_literal:
                case list() | tuple() | dict():
                    self._process_response(response)
                case _:
                    break

        return response

    def __call__(self, user_input):
        return self.loop_until_satisfactory(user_input)
